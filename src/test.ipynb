{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input,Conv2D,BatchNormalization,Activation,Flatten,Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "import cv2 as cv\n",
    "import gym\n",
    "import gym_chrome_dino\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "\n",
    "%run Image_Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C:\n",
    "    \n",
    "    def __init__(self,input_shape,output_shape,gamma,threads,environment):\n",
    "        self.input_shape_ = input_shape\n",
    "        self.output_shape_ = output_shape\n",
    "        self.gamma_ = gamma\n",
    "        self.threads_ = threads\n",
    "        self.environment_ = environment\n",
    "        \n",
    "        self.actor_,self.critic_ = self.__build_actor_critic__()\n",
    "        \n",
    "        self.sess_ = tf.InteractiveSession()\n",
    "        K.set_session(self.sess_)\n",
    "        self.sess_.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __build_actor_critic__(self):\n",
    "        input_ = Input(shape=self.input_shape_)\n",
    "        conv = Conv2D(filters=32,kernel_size=(8,8),padding=\"valid\",strides=(2,2))(input_)\n",
    "        conv = BatchNormalization()(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = Conv2D(filters=32,kernel_size=(8,8),padding=\"valid\",strides=(2,2))(conv)\n",
    "        conv = BatchNormalization()(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = Conv2D(filters=32,kernel_size=(8,8),padding=\"valid\",strides=(2,2))(conv)\n",
    "        conv = BatchNormalization()(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        \n",
    "        flat = Flatten()(conv)\n",
    "        \n",
    "        dense = Dense(units=512)(flat)\n",
    "        dense = Activation('relu')(dense)\n",
    "        dense = Dense(units=128)(dense)\n",
    "        dense = Activation('relu')(dense)\n",
    "        \n",
    "        policy = Dense(units=self.output_shape_, activation='softmax')(dense)\n",
    "        value = Dense(units=1,activation='linear')(dense)\n",
    "        \n",
    "        actor = Model(inputs=input_,outputs=policy)\n",
    "        critic = Model(inputs=input_,outputs=value)\n",
    "        \n",
    "        actor._make_predict_function()\n",
    "        critic._make_predict_function()\n",
    "        \n",
    "        return actor,critic\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __update_actor__(self):\n",
    "        action = K.placeholder(shape=(None, self.output_shape_))\n",
    "        advantages = K.placeholder(shape=(None, ))\n",
    "        policy = self.actor_.output\n",
    "        \n",
    "        good_prob = K.sum(action * policy, axis=1)\n",
    "        eligibility = K.log(good_prob+1e-10) * K.stop_gradient(advantages)\n",
    "        loss = -K.sum(eligibility)\n",
    "        \n",
    "        gradient = RMSprop()\n",
    "        updates = gradient.get_updates(self.actor_.trainable_weights,[],loss)\n",
    "        train = K.function([self.actor_.input,action,advantages],[self.actor_.output],updates=updates)\n",
    "        \n",
    "        return train\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __update_critic__(self):\n",
    "        discounted_rewards = K.placeholder(shape=(None, ))\n",
    "        value = self.critic_.output\n",
    "        \n",
    "        loss = K.mean(K.square(discounted_rewards - value))\n",
    "        \n",
    "        gradient = RMSprop()\n",
    "        updates = gradient.get_updates(self.critic_.trainable_weights, [], loss)\n",
    "        train = K.function([self.critic_.input, discounted_rewards],[self.critic_.output],updates=updates)\n",
    "        \n",
    "        return train\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, max_episode, total_reward):\n",
    "        dinos = [Dino(self.input_shape_, self.output_shape_, self.environment_, self.actor_, self.critic_,\n",
    "                     self.__update_actor__(), self.__update_critic__(), self.gamma_, self.sess_,\n",
    "                      max_episode, total_reward) for i in range(self.threads_)]\n",
    "        for dino in dinos:\n",
    "            dino.start()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Dino(threading.Thread):\n",
    "    \n",
    "    def __init__(self,input_shape,output_shape,environment,global_actor,global_critic,\n",
    "                update_actor,update_critic,gamma,sess,max_episode,total_rewards):\n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        \n",
    "        self.input_shape_ = input_shape\n",
    "        self.output_shape_ = output_shape\n",
    "        self.environment_ = environment\n",
    "        self.global_actor_ = global_actor\n",
    "        self.global_critic_ = global_critic\n",
    "        self.update_actor_ = update_actor\n",
    "        self.update_critic_ = update_critic\n",
    "        self.gamma_ = gamma\n",
    "        self.sess_ = sess\n",
    "        self.max_episode_ = max_episode\n",
    "        self.total_rewards_ = total_rewards\n",
    "        \n",
    "        self.states_, self.actions_, self.rewards_ = deque(),deque(),deque()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        e = 0\n",
    "        while e != self.max_episode_:\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            state = self.environment_.reset()\n",
    "            state = self.__process_initial_state__(state)\n",
    "            while not done:\n",
    "                action = self.__action__(state)\n",
    "                next_state,reward,done,info = self.environment_.step(action)\n",
    "                total_reward += reward\n",
    "                state = self.__process_new_state__(state,next_state)\n",
    "                if done:\n",
    "                    e += 1\n",
    "                    print(\"episode: {}/{}\\n reward: {}\".format(e+1,self.max_episode,total_reward))\n",
    "                    self.total_rewards.append(total_reward)\n",
    "                    self.__update_actor_critic__(total_reward > 600)\n",
    "        self.__plot_total_rewards__()\n",
    "        self.__save_weights__(\"{}\".format(self.max_episodes_))\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __process_initial_state__(self, state):\n",
    "        canny = img_processing(\"canny\",self.input_shape_[0],self.input_shape_[0],state)\n",
    "        stacked = stack_images([canny,canny,canny,canny],4)\n",
    "        reshaped_state = reshape_to(4,stacked)\n",
    "        return reshaped_state\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __process_new_state__(self, stacked_state, next_state):\n",
    "        canny = img_processing(\"canny\",self.input_shape_[0],self.input_shape_[0],next_state)\n",
    "        new_stacked = fifo_images(next_state,stacked_state)\n",
    "        return new_stacked\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __action__(self,state):\n",
    "        policy = self.global_actor_.predict(state)\n",
    "        a = np.random.choice(self.output_shape_,1,p=policy)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __discounted_rewards__(self, rewards, done):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        R = self.global_critic_.predict(self.states_[-1]) * (1-int(done))\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + self.gamma_ * R\n",
    "            discounted_rewards[i] = R\n",
    "        \n",
    "        return discounted_rewards\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __update_actor_critic__(self, done):\n",
    "        states = self.states_\n",
    "        actions = self.actions_\n",
    "        value = self.global_critic_.predict(states)\n",
    "        discounted_rewards = self.__discounted_rewards__(self.rewards_, done)\n",
    "        advantages = discounted_rewards - value\n",
    "        \n",
    "        self.update_actor_(states,actions,advantages)\n",
    "        self.update_critic_(states,discounted_rewards)\n",
    "        self.__clear_deque__()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __clear_deque__(self):\n",
    "        self.states_.clear()\n",
    "        self.actions_.clear()\n",
    "        self.rewards_.clear()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __remember__(self,state,action,reward):\n",
    "        self.states_.append(state)\n",
    "        act = np.zeros(self.output_shape_)\n",
    "        act[action] = 1\n",
    "        self.actions_.append(act)\n",
    "        self.rewards_.append(reward)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __save_weights__(self,name):\n",
    "        self.global_actor_.save_weights(name+\"_actor.h5\")\n",
    "        self.global_critic_.save_weights(name+\"_critic.h5\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __plot_total_rewards__(self):\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.title(\"Reward / Episode\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"rewards\")\n",
    "        plt.plot(self.total_rewards_, label=\"rewards\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"ChromeDinoNoBrowser-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (80,80,4)\n",
    "output_shape = environment.action_space.n\n",
    "gamma = 1\n",
    "threads = 1\n",
    "\n",
    "max_episode = 100\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a3c = A3C(input_shape, output_shape, gamma, threads, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-2-ef37eab7c6f6>\", line 124, in run\n",
      "    action = self.__action__(state)\n",
      "  File \"<ipython-input-2-ef37eab7c6f6>\", line 154, in __action__\n",
      "    policy = self.global_actor_.predict(state)\n",
      "  File \"C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\keras\\engine\\training.py\", line 1462, in predict\n",
      "    callbacks=callbacks)\n",
      "  File \"C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 324, in predict_loop\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3473, in __call__\n",
      "    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n",
      "  File \"C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3410, in _make_callable\n",
      "    callable_fn = session._make_callable_from_options(callable_opts)\n",
      "  File \"C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1505, in _make_callable_from_options\n",
      "    return BaseSession._Callable(self, callable_options)\n",
      "  File \"C:\\Users\\CSY\\Anaconda3\\envs\\env1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1460, in __init__\n",
      "    session._session, options_ptr)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensor input_1:0, specified in either feed_devices or fetch_devices was not found in the Graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a3c.train(max_episode, total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(5)\n",
    "b = np.array([[2,3,4,5,6]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1,  0, -1, -2],\n",
       "       [ 3,  2,  1,  0, -1],\n",
       "       [ 4,  3,  2,  1,  0],\n",
       "       [ 5,  4,  3,  2,  1],\n",
       "       [ 6,  5,  4,  3,  2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
